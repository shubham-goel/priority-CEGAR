\documentclass{sig-alternate}[9pt]

	\usepackage{times}
	\usepackage{amssymb}
	%\usepackage{amsmath}
	\usepackage{dsfont}
	\usepackage{xspace}
	\usepackage{graphicx}
	\usepackage{mathabx}
	\let\proof\relax 
	\let\endproof\relax
	\usepackage{amsthm}

	\usepackage{multirow}

	\usepackage{algpseudocode}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}


	%\renewcommand{\baselinestretch}{1.15}

	\newcommand{\zug}[1]{\langle #1  \rangle}
	\newcommand{\set}[1]{\{ #1  \}}
	\newcommand{\stam}[1]{}
	\newcommand{\bft}{\mbox{\em \bf true}}
	\newcommand{\bff}{\mbox{\em \bf false}}
	%\newcommand{\track}[1]{{\textcolor{red}{#1}}}



	\newcommand{\A}{{\cal A}}
	\newcommand{\B}{{\cal B}}
	\newcommand{\C}{{\cal C}}
	\newcommand{\D}{{\cal D}}
	\newcommand{\G}{{\cal G}}
	\renewcommand{\L}{{\cal L}}
	\newcommand{\N}{{\cal N}}
	\newcommand{\M}{{\cal M}}
	\newcommand{\T}{{\cal T}}

	\newcommand{\Nat}{\mathds{N}}
	\newcommand{\Rat}{\mathds{Q}}
	\newcommand{\Qpos}{\Rat^{\geq 0}}
	\newcommand{\Real}{\mathds{R}}
	\newcommand{\Whole}{\mathds{Z}}

	\newcommand{\Prot}{\mbox{\sc Prot}}


	\newcommand{\osigma}{\overline{\sigma}}

	\newcommand{\used}{\eta}
	\newcommand{\usedb}{\kappa}

	\newcommand{\PO}{Player~$1$\xspace}
	\newcommand{\PT}{Player~$2$\xspace}
	\newcommand{\PR}{Player~$3$\xspace}
	\newcommand{\PF}{Player~$4$\xspace}

	\newcommand{\FP}{\text{FP}}
	\newcommand{\SP}{\text{SP}}
	\newcommand{\Prc}{\text{Prc}}



	\newcommand{\ff}{\texttt{f\!f}\xspace}
	\renewcommand{\tt}{\texttt{t\!t}\xspace}


	\newcommand{\start}{\mbox{start}\xspace}
	\newcommand{\eend}{\mbox{end}\xspace}




	\newtheorem{theorem}{Theorem}[section]
	\newtheorem{proposition}[theorem]{Proposition}
	\newtheorem{property}[theorem]{Property}
	\newtheorem{corollary}[theorem]{Corollary}
	\newtheorem{observation}[theorem]{Observation}
	\newtheorem{lemma}[theorem]{Lemma}
	\newtheorem{claim}[theorem]{Claim}
	\newtheorem{fact}[theorem]{Fact}
	\newtheorem{definition}{Definition}[section]
	\newtheorem{algorithm}{Algorithm}[section]
	\theoremstyle{definition}
	\newtheorem{example}{Example}[section]
	\newtheorem{rmrk}[theorem]{Remark}
	%\newtheorem{exmpl}[theorem]{Example}
	%\newenvironment{example}{\begin{exmpl}\rm}{\hspace{\stretch{1}\hfill\qed}\end{exmpl}}

	\newenvironment{remark}{\begin{rmrk}\rm}{\hspace{\stretch{1}}\end{rmrk}}

	\def\eod{\vrule height 6pt width 5pt depth 0pt}
	\renewenvironment{proof}{\noindent {\bf Proof:} \hspace{.677em}} {\hspace*{\fill}{\eod}}



	\begin{document}
	\setcopyright{acmcopyright}

	% DOI
	\doi{10.475/123_4}

	% ISBN
	\isbn{123-4567-24-567/08/06}

	%Conference
	\conferenceinfo{EMSOFT}{October 2--7, 2016, Pittsburgh, PA, USA}

	\acmPrice{\$15.00}

	%
	% --- Author Metadata here ---
	\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
	%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
	%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
	% --- End of Author Metadata ---

	\title{Synthesizing Time-Triggered Schedules for Switched Networks with Faulty Links}
	\author{Authors' names deliberately omitted for blind review}

	%\author{
	%\alignauthor
	%Guy Avni\\
	%       \affaddr{IST Austria}\\
	% %      \affaddr{Wallamaloo, New Zealand}\\
	%       \email{guy.avni@ist.ac.at}
	%\alignauthor
	%Shibashis Guha\\
	%       \affaddr{Hebrew University}\\
	%   %    \affaddr{Wallamaloo, New Zealand}\\
	%       \email{shibashis@cse.iitd.ac.in}
	%\alignauthor
	%Guillermo Rodriguez-Navas\\
	%       \affaddr{MÃ¤lardalen University}\\
	%     %  \affaddr{Wallamaloo, New Zealand}\\
	%       \email{guillermo.rodriguez-navas@mdh.se}
	%}



	\maketitle


\section{evaluation}
	% We have implemented the CEGAR loop for finding a $(k,\ell)$-resistant schedule, which is described in Section~\ref{sec:CEGAR}, assuming that the switches run the two-path protocol. Our implementation is in Python and relies on Z3 \cite{MB08} as an SMT solver. We ran our experiments on a personal computer, Intel Core i3 quad core 3.40 GHz processor, with 8 GB memory.

	We have implemented Weighted Model Counting, Iterative approach and the Monte-Carlo Approach for scoring forwarding schemes, which are described in Section 2 and Section 3. Our implementations are in Python and rely on counting tools, both unweighted (sharp-Sat, cryptominsat) and weighted (WeightMC). We ran our experiments on a personal computer, Intel Core i3 quad core 3.40 GHz processor, with 8 GB memory.
	\noindent{\bf Network and {\em forwarding scheme} generation}
		We evaluate the algorithm on networks that were generated randomly by the Python library Networkx
		 \cite{HSS08}.
		The networks are generated in the following manner.
		We fix the number of vertices, edges, and messages.
		We generate a random directed graph.
		%We consider relatively dense graphs, where  % This may not be considered dense.
		%The number of edges ranges from $1.2$ to $1.5$ times the number of vertices.
		We consider relatively dense graphs, where the number of edges are approximately 2 times the number of vertices.
		Once we have a graph, we randomly select a source and a target for each message,
		while guaranteeing that these are different nodes.
		We refer to a graph and messages with the source and the target vertices, 
		as the {\em setting}.
		The {\em forwarding scheme} is generated for a setting deterministically.

		We find the edge priorities for each message. We note that the problem of finding ``good'' edge priorities is a crucial component of the protocol. It is a challenging problem both theoretically and practically, and it is out of the scope of this paper. We now describe the edge priority-finding algorithm we use. We start with a simple approach.

		Consider a message $m \in \M$. Recall that $s(m)$ and $t(m)$ denote the source vertex
		and the target vertex of message $m$, respectively. For finding the edge priorities at different vertices, we find first and second paths for the message $m$. For all edges outgoing from vertex $v$, an edge in the first path will have higher priority than one in the second path. If $v$ doesn't  lie on any of the first of second paths, $m$ cannot be forwarded from $v$.
		The first path is the shortest path between $s(m)$ and $t(m)$.
		To find the second path, we remove the edges that the first path traverses.
		For each vertex $v$ on the first path,
		we set the second path from $v$ to be the shortest path from $v$ to $t(m)$ in the new graph, if it exists. 
		The problem with this simple approach is that the second paths are of similar lengths for the different messages, making it hard to calculate the {\em Score}. Thus, we improve the approach by, intuitively, having messages avoid heavily loaded edges when selecting the two paths. Technically, the graph is now weighted. The weight of  an edge refers to the number of times it appears in a first or second path. We construct the weights in the graph incrementally. The weights start from $0$. We order the messages arbitrarily and choose them one by one. When a message is chosen, it selects its first and second paths similarly to the above (only running Dijkstra's algorithm rather than a BFS for finding shortest paths). The weight of every edge on the first and second path is incremented by $1$.

% --------------------Parse  Pointer--------------------------
		Next, we decide message priorities for each vertex. Upon implementing different heuristics for deciding message priorities, we see that different heuristics result in different $Scores$. For the purpose of evaluation, we use the {\em longest path first} where a message with a longer first path is prioritized first. 
		As a last step before finding the {\em Score}, we need to decide the {\em forwarding algorithm}. Though both {\em Hot-Potato} and {\em Fallback} approaches have been implemented, we use the {\em Hot-Potato} approach, which is described in Example 1.2 


	% \noindent{\bf Execution time measurements}
	% 	%%% New -  Guillermo %%%%
	% 	We elaborate on the pseudo-code that is described in Section~{sec:CEGAR}. An execution starts with an initialization phase (or \emph{Setup}), in which the method {\sc $\exists$Sched}, implementing the program described in Theorem~{thm:exists-schedule}, is called; followed by the iterative phase, which we refer to as the CEGAR loop.
	% 	% which an iterative phase that concludes when a $(k,\ell)$-resistant schedule has been obtained or no such schedule exists. 
	% 	Method {\sc $\exists$Sched} returns the initial schedule to be used in the CEGAR loop.
	% 	An iteration starts with a call to the method {\sc IsResist}, which is an implementation of the program that is described in Theorem~{thm:worst-faults}. Method {\sc IsResist}$(S,k,\ell)$ returns $\tt$ and concludes the iterations if schedule $S$ is $(k,\ell)$-resistant, or otherwise it returns a fault sequence that witnesses the non-resistance of $S$. If the schedule is not $(k,\ell)$-resistant, the second method that is run in the CEGAR iteration is {\sc $\exists$SchedGivConf}, which implements the optimization that is described in Theorem~{thm:optimization}. A call to {\sc $\exists$SchedGivConf}$(F, C_i)$ returns $\tt$ if it is possible to recover from the configuration $C_i$, assuming the sequence of faults $F$ occurs. This check is performed for every time point $0 \leq i \leq t-1$ such that the $i$-constraint is not $\tt$. If for some time $i$, it finds that it is not possible to recover from $C_i$ then $\neg \psi^i$ is added to the SMT program. The iteration ends with a call to the SMT solver. If it is SAT, then we generate a schedule for the next iteration, whereas if it is UNSAT, we terminate and return that no $(k,\ell)$-resistant schedule exists.


	% 	Our goal in the implementation is to evaluate the performance of our CEGAR loop rather than focusing on scalability. In our experiments we considered small settings (S) with $30$ vertices, $50$ messages, $40$ edges, a timeout of $10$, and $\ell = 45$ while considering different values for $k$, and a larger setting (L) with $100$ vertices, $150$ messages, $200$ edges, a timeout of $12$, and $\ell=150$. For each setting, we measured three different aspects: the time needed for the Setup phase, the average time needed to execute one iteration and the average number of iterations required before finding a $(k,\ell)$-resistant schedule. The iteration execution times were measured first without applying the optimization, in what we call the Basic loop, and then with the optimization. Our intention was understanding the contributions in terms of execution time of the different methods of the approach and assess the improvement caused by the optimization.


	% 	%So, we evaluated on relatively small networks to keep a low running time for the iterations.

	% 	%Since the running time of an iteration varies significantly we list in Table~{tab:time} the running times of the different components of the program. 

	% 	Table~{tab:time} shows the execution time of the Setup phase and of the basic iteration (I-Bas) and the optimized iteration (I-Opt). The values have been averaged over 3-5 executions. %a small number of executions, between 3 and 5.

	% 	%
	% 	%The basic CEGAR loop  consist of a call to {\sc IsResist} and an SMT call to find the next schedule. The running time of the first method takes the vast majority of the running time. It is possible to add an optimization and/or a call to the method {\sc Calc-$\ell$} to the basic loop. All the values are averages of $3-5$ runs.


	% \begin{table}[htbp]
	% 	\begin{center}
	% 	\begin{tabular}{|c|c|c|c|c|}
	% 	\hline
	% 	Size: $k$& Setup (sec) & I-Bas (sec) & I-Opt (sec)\\
	% 	\hline
	% 	\hline
	% 	S: $k=1$& {\small $2.03$} & {\small $11.3$} & {\small $27.17$}  \\
	% 	\hline
	% 	S: $k=2$ & {\small $2.04$} & {\small $17.64$} & {\small $31.5$} \\
	% 	\hline
	% 	S: $k=3$ & {\small $2.07$} & {\small $17.42$} & {\small $31.08$}\\
	% 	\hline
	% 	L: $k=1$ & {\small $11.92$} &{\small $142.18$} & {\small $261.91$} \\
	% 	\hline
	% 	\end{tabular}
	% 	\end{center}
	% 	\stam{
	% 	\begin{table}[htbp]
	% 	\begin{center}
	% 	\begin{tabular}{|c|c|c|c|c|}
	% 	\hline
	% 	Size: $k$& Pre-run &I_{Bas} & I_{Opt} & B+{\sc Calc-$\ell$}\\
	% 	\hline
	% 	\hline
	% 	S: $k=1$& {\small $2.03$} & {\small $11.28+0.02$} & {\small $15.1$ $(15.87)$} & {\small $7.27$} \\
	% 	\hline
	% 	S: $k=2$ & {\small $2.04$} & {\small $17.6+0.04$} & {\small $12.86$ $(13.86)$} & {\small $68.6$} \\
	% 	\hline
	% 	S: $k=3$ & {\small $2.07$} & {\small $17.4+0.02$} & {\small $12.93$ $(13.66)$} & {\small $823.22$} \\
	% 	\hline
	% 	L: $k=1$ & {\small $11.92$} &{\small $142.06+0.12$} & {\small $112.53$ $(119.73)$} & {\small $9386$} \\
	% 	\hline
	% 	\end{tabular}
	% 	\end{center}
	% 	}
	% 	\vspace{-0.4cm}
	% 	\caption{Execution times of the Setup phase and the iterations (without and with optimization) of the CEGAR loop.}%S and L represent small and large networks, respectively. All times are in seconds.}
	% 	\label{tab:time}
	% \end{table}

	% It is interesting to observe that the execution time of one iteration with optimization is sensibly longer than the one of a basic iteration. Nevertheless, considering the overall time needed for finding a $(k,\ell)$-resistant schedule, the optimization yields much better results than the basic loop. In Table~{tab:iter} we show the average number of iterations till the CEGAR loop terminates when the optimization is used: typically, the CEGAR loop terminates after one or two iterations.  If the optimization finds that it is not possible to recover from the first configuration $C_0$, it adds the constraint \ff to the program, thereby indicating that no $(k,\ell)$-resistant schedule exists
	% and terminating the CEGAR loop. We find that this generally occurs in the first few iterations. % of the CEGAR loop.
	% %This holds both for the initial run described above with $\ell=m$, and for the second run with $\ell = \ell_S +1$.  
	% In contrast, when running the CEGAR loop with no optimization, the performance is poor. The run either finds the first schedule to be $(k,\ell)$-resistant or it does not terminate.


	% % we observe that {\sc $\exists$SchedGivConf} performs very well for random settings as we illustrate in Table~{tab:iter}, which depicts the average number of iterations till the CEGAR loop terminates when the optimization is used.
	% %Typically, the CEGAR loop terminates after one or two iterations. 
	% %If the optimization finds that it is not possible to recover from the first configuration $C_0$,
	% %it adds the constraint \ff to the program, thereby 
	% %indicating that no $(k,\ell)$-resistant schedule exists
	% %and terminating the CEGAR loop.
	% %We find that this typically occurs in the first few iterations.% of the CEGAR loop.
	% %%This holds both for the initial run described above with $\ell=m$, and for the second run with $\ell = \ell_S +1$.  
	% %When running the CEGAR loop with no optimization,% and the same setting and parameters, 
	% %the performance is poor. The run either finds the first schedule to be $(k,\ell)$-resistant or it does not terminate.


	% \begin{table}[htbp]
	% \begin{center}
	% \begin{tabular}{|c|c|c|}
	% \hline
	% Size: $k$& Number of iterations\\% & no OPT \\
	% \hline
	% \hline
	% S: $k=1$& 1.4  \\
	% \hline
	% S: $k=2$ & 1.2\\
	% \hline
	% S: $k=3$ & 1.4\\
	% \hline
	% S: $k=4$ & 1 \\
	% \hline
	% L: $k=1$ & 1.3 \\
	% \hline
	% \end{tabular}
	% \end{center}
	% \vspace{-0.4cm}
	% \caption{The average number of iterations till the CEGAR loop terminates.}
	% \label{tab:iter}
	% \end{table}





	% %%% OLD - Before Guillermo %%%%

	% %Our goal in the implementation is to evaluate the performance of our CEGAR loop rather than focusing on scalability. 
	% %So, we evaluated on relatively small networks to keep a low running time for the iterations. Since the running time of an iteration varies significantly we list in Table~{tab:time} the running times of the different components of the program. We considered small settings (S) with $30$ vertices, $50$ messages, $40$ edges, a timeout of $10$, and $\ell = 45$ while considering different values for $k$, and a larger setting (L) with $100$ vertices, $150$ messages, $200$ edges, a timeout of $12$, and $\ell=150$. The basic CEGAR loop, referred to as B, consist of a call to {\sc IsResist} and an SMT call to find the next schedule. The running time of the first method takes the vast majority of the running time. It is possible to add an optimization and/or a call to the method {\sc Calc-$\ell$} to the basic loop. All the values are averages of $3-5$ runs.
	% %
	% %
	% %\begin{table}[htbp]
	% %\begin{center}
	% %\begin{tabular}{|c|c|c|c|c|}
	% %\hline
	% %Size: $k$& Pre-run &B & B+Opt & B+{\sc Calc-$\ell$}\\
	% %\hline
	% %\hline
	% %S: $k=1$& {\small $2.03$} & {\small $11.3$} & {\small $27.17$} & {\small $7.27$} \\
	% %\hline
	% %S: $k=2$ & {\small $2.04$} & {\small $17.64$} & {\small $31.5$} & {\small $68.6$} \\
	% %\hline
	% %S: $k=3$ & {\small $2.07$} & {\small $17.42$} & {\small $31.08$} & {\small $840.64$} \\
	% %\hline
	% %L: $k=1$ & {\small $11.92$} &{\small $142.18$} & {\small $261.91$} & {\small $9528.18$} \\
	% %\hline
	% %\end{tabular}
	% %\end{center}
	% %\stam{
	% %\begin{table}[htbp]
	% %\begin{center}
	% %\begin{tabular}{|c|c|c|c|c|}
	% %\hline
	% %Size: $k$& Pre-run &B & B+Opt & B+{\sc Calc-$\ell$}\\
	% %\hline
	% %\hline
	% %S: $k=1$& {\small $2.03$} & {\small $11.28+0.02$} & {\small $15.1$ $(15.87)$} & {\small $7.27$} \\
	% %\hline
	% %S: $k=2$ & {\small $2.04$} & {\small $17.6+0.04$} & {\small $12.86$ $(13.86)$} & {\small $68.6$} \\
	% %\hline
	% %S: $k=3$ & {\small $2.07$} & {\small $17.4+0.02$} & {\small $12.93$ $(13.66)$} & {\small $823.22$} \\
	% %\hline
	% %L: $k=1$ & {\small $11.92$} &{\small $142.06+0.12$} & {\small $112.53$ $(119.73)$} & {\small $9386$} \\
	% %\hline
	% %\end{tabular}
	% %\end{center}
	% %}
	% %\vspace{-0.4cm}
	% %\caption{The running times of the components in an iteration of the CEGAR loop in seconds. S and L represent small and large networks, respectively. }
	% %\label{tab:time}
	% %\end{table}
	% %
	% %Before the CEGAR loop, an initial schedule is found by calling the method {\sc $\exists$Sched}, which implements the program that is described in Theorem~{thm:exists-schedule}. An iteration of the CEGAR loop starts with a call to the method {\sc IsResist}, which is an implementation of the program that is described in Theorem~{thm:worst-faults}. Calling {\sc IsResist}$(S,k,\ell)$ returns $\tt$ if schedule $S$ is $(k,\ell)$-resistant, and otherwise the method returns a fault sequence that witnesses the non-resistance of $S$. Assuming the schedule is not $(k,\ell)$-resistant, the second method that is run in the CEGAR loop is {\sc $\exists$SchedGivConf}, which implements the optimization that is described in Theorem~{thm:optimization}. A call to {\sc $\exists$SchedGivConf}$(F, C_i)$ returns $\tt$ if it is possible to recover from the configuration $C_i$, assuming the sequence of faults $F$ occurs. 
	% %We perform this check for every time point $0 \leq i \leq t-1$
	% %such that the $i$-constraint is not $\tt$. 
	% %If for some time $i$, we find that it is not possible to recover from $C_i$,
	% %we add $\neg \psi^i$ to the SMT program.
	% %We solve a final SMT program to generate a new schedule for the next iteration. 
	% %
	% %
	% %
	% %
	% %%\multirow{2}{*}{\parbox{1.5cm}{\centering Activation\\costs}}
	% %
	% %
	% %
	% %%We used a relatively dense network with $30 \leq n \leq 50$ nodes, and between $1.2n$ and $1.5n$ edges. The number of messages we use is equal to the number of edges. In such a network, the number of edges that appears in the first path is typically between $1$ and $7$. Using the naive path-finding algorithm, the second paths are of the same length as the first paths, and with the smarter algorithm the sizes of the second path vary and is more than that of the first path. Let $p$ be the length of the longest first path. In our experiments, we use a timeout that is slightly higher than $p$, e.g., between $p+1$ and $p+3$. The running time of an iteration with these parameters varies, but typically does not exceed roughly $30$ seconds. We elaborate on what an iteration consists of below. We also ran on larger networks with $100$ nodes, $200-250$ edges, and $150$ messages. An iteration does not exceed $300$ seconds for these parameters.
	% %
	% %
	% %
	% %We observe that for the small networks, the majority of the running time is spent in the setup of the program, i.e., initializing the variables and constraints of the program, rather than in the solution of the program. We amortized some of the programs we construct by re-using most of the constraints. As we show in Table~{tab:time}, the amortization significantly decreases the running time of the subsequent calls to {\sc $\exists$SchedGivConf}.
	% %
	% %note that in many of our programs, most of the constraints are very similar. We reuse some of our Z3 solvers, thereby amortizing the running time. For example, the first call to {\sc $\exists$SchedGivConf} takes $16$ seconds and the following calls take much less than $1$ second. 

	% \noindent{\bf Additional results}
	% We found the following workflow convenient and useful in practice. We assume the network is typically fixed as well as the number of faults $k$. %The parameter that the designer has control on are the number of messages $m$ that are sent, and on the guarantee of messages received $\ell$. 
	% For a fixed setting, and fixed $k$, we refer to the {\em best} $\ell$ as the highest $\ell$ such that a $(k,\ell)$-resistant schedule exists and no $(k,\ell+1)$-resistant schedule exists.  
	% If the best $\ell$ is too low, the designer can use redundancy and increase $m$. %Initially, we fix $m$. 
	% We observe that it is convenient to run the CEGAR loop with $\ell = m$. Since we assume $k>0$, it is rare that a $(k,m)$-resistant schedule exists. But, an initial schedule $S$ is typically found. 

	% We implemented a method, which we call {\sc Calc-$\ell$}, that finds the largest $\ell_S$ such that $S$ is $(k, \ell_S)$-resistant.
	% This $\ell_S$ gives us a guess on the best $\ell$. The method we implement is a binary search that performs calls to the method {\sc IsResist}. That is, for $0 \leq c \leq \ell$, we call {\sc IsResist}$(S,k, c)$. If it returns $\tt$, then $S$ is $(k,\ell)$-resistant and we increase $c$, and otherwise we decrease $c$. As mentioned above, the programs in the binary search differ in exactly one constraint; the constraint that guarantees that less than $\ell$ messages arrive on time. Amortizing the construction helps, but the running time of this method is significant ($\sim823s$ for S:$k=3$ and $\sim9386s$ for L:$k=1$) mostly due to calls to {\sc IsResist} that are UNSAT.

	% We observe one phenomenon that is surprising for us. Often the first schedule is the ``best'' schedule. That is, if $S$ is the first schedule that is found and its guarantee is $\ell_S$, then $\ell_S$ is often the best $\ell$. %This is independent of the path-finding algorithm that is used while it is more common with the naive algorithm. 
	% To verify that $\ell_S$ is indeed the best $\ell$,  we run the CEGAR loop with the same setting and set $\ell = \ell_S +1$. The reason behind this result remains unknown and will require further investigation.

	% %We observe that sometimes, with optimization, the CEGAR loop times out. Moreover, we notice that in these cases, the same fault sequence is found repeatedly.% found in most iterations. We conjecture that it is possible to improve our optimization or use the existing one in order to find the next schedule in a move clever manner in order to avoid these cases.



\end{document}